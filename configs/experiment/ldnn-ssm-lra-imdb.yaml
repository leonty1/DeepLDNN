defaults:
  - /pipeline: imdb
  - /model: ldnn-ssm
  - override /scheduler: plateau

model:
  dropout: 0.1
  n_layers: 6
  prenorm: true
  d_model: 128
  norm: batch
  residual: R
  layer:
    head: 128
    bidirectional: true
    channel: 1
    conj_sym: false
    keep_d_state: true
    discrete_method: zoh  #'zoh'
    d_s: 128
    Lambda_init: hippo
    B_init: hippo
    C_init: trunc_normal
    D_init: one
    t_init: uniform
    dt_min: 0.001
    dt_max: 0.1
    d_dt_is_n: true
    version_A_neg: clip
    version_A_imag: same
    version_B_imag: same
    version_C_imag: same
    max_kernel_length:
    D_value: 1
    trainable:
      log_dt: true
      Lambda: true
      B: true
      C: true
      D: true
    lr:
      log_dt: 0.001
      Lambda: 0.001
      B: 0.001
      C: 0.004
      D: 0.001
    activation:  #leakyrelu #leakyrelu #S5  half_glu2  full_glu   half_glu1
    acttivation_post: half_glu2  # activation after Mixer

dataset:
  l_max: 4096
  level: char

loader:
  batch_size: 16    #batch 16, 25000 intotal, 12500 training samples, 781 steps per epoch =

decoder:
  mode: pool

optimizer:
  lr: 0.004
  weight_decay: 0.07

scheduler:
#  plateau
  patience: 5
  factor: 0.5
  #cosine_warmup
#  num_warmup_steps: 200
#  num_training_steps: 100000 #12500    #250 steps * epoch
#  milestones: [10,20,30,40,50]
#  gamma: 0.5


trainer:
  max_epochs: 120    #10000 steps 40 epochs
  resume_from_checkpoint: C:\GroupMembers\Liang20\stprediction\Code_SSM\dss-main\outputs\2024-05-10\08-38-21\checkpoints\val\accuracy.ckpt
#  accelerator: 'gpu'
#  devices: '1'

train:
  seed: 1112

