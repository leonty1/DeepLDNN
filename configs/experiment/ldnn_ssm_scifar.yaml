
defaults:
  - /pipeline: cifar
  - /model: ldnn-ssm
  - override /scheduler: plateau

model:
  dropout: 0.1
  n_layers: 6
  prenorm: true
  d_model: 256
  norm: batch
  residual: R
  layer:
    head: 64
    bidirectional: true
    channel: 1
    conj_sym: true
    keep_d_state: true
    discrete_method: zoh  #'zoh'
    d_s: 512
    Lambda_init: hippo #hippo #hippo ['uniform', 'kaiming_uniform', 'xavier_uniform', 'normal', 'kaiming_normal', 'xavier_normal', 'trunc_normal', 'zero', 'one', 'half']
    B_init: hippo #hippo
    C_init: trunc_normal
    D_init: one
    t_init: uniform   #['uniform', 'trunc_normal']
    dt_min: 0.001
    dt_max: 0.1
    d_dt_is_n: true
    version_A_neg: clip              # constrain A - ['softplus', 'sigmoid', 'Gaussian',  'clip']
    version_A_imag: same             #['softplus', 'sigmoid', 'Gaussian',  'clip', 'zero', 'negtive', ['identity', None, 'same']]
    version_B_imag: zero
    version_C_imag: zero
    max_kernel_length:
    D_value: 1
    trainable:
      log_dt: true
      Lambda: true
      B: true
      C: true
      D: true
    lr:
      log_dt: 0.001
      Lambda: 0.001
      B: 0.005
      C: 0.005
      D: 0.001
    activation:  #leakyrelu #leakyrelu #half_glu1  half_glu2
    acttivation_post: half_glu1  # activation after Mixer

dataset:
  permute: null
  grayscale: False
  augment: true

loader:
  batch_size: 50    #30,000 samples, 450 per epoch for training

optimizer:
  lr: 0.005
  weight_decay: 0.05

scheduler:
  patience: 8
  factor: 0.6



trainer:
  max_epochs: 300

train:
  seed: 10
